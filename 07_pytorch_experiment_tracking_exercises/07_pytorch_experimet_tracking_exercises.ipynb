{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77de26a6",
   "metadata": {},
   "source": [
    "# 07. PyTorch Experiment Tracking Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d453d3f",
   "metadata": {},
   "source": [
    "Resource: https://www.learnpytorch.io/07_pytorch_experiment_tracking/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9f89d",
   "metadata": {},
   "source": [
    "## 0. Get imports and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ad1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f812147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eabac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import get_data, setup_data, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c011baaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f67363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b63f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(\n",
    "    experiment_name: str,\n",
    "    model_name: str,\n",
    "    extra: str=None,\n",
    "):\n",
    "    \"\"\"Creates a torch.utils.tensorboard.SummaryWriter() instance saving to a specific log_dir\"\"\"\n",
    "\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    timestampe = datetime.now().strftime(\"%y-%m-%d\")\n",
    "    \n",
    "    if extra:\n",
    "        log_dir = os.path.join(\"runs\", timestampe, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestampe, experiment_name, model_name)\n",
    "    \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97a070c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import torch.utils.tensorboard\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "    writer: torch.utils.tensorboard.writer.SummaryWriter\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Trains and test PyTorch model\"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = engine.train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc = engine.test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "        ### New: Use the writer parameter to track experiments ###\n",
    "        # See if there's a writer, if so, log to it\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(\n",
    "                main_tag=\"Loss\", \n",
    "                tag_scalar_dict={\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"test_loss\": test_loss\n",
    "                },\n",
    "                global_step=epoch\n",
    "            )\n",
    "            \n",
    "            writer.add_scalars(\n",
    "                main_tag=\"Accuracy\", \n",
    "                tag_scalar_dict={\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"test_acc\": test_acc\n",
    "                }, \n",
    "                global_step=epoch\n",
    "            )\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf13d5",
   "metadata": {},
   "source": [
    "## 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "510dc650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi_10_percent exists...\n",
      "Data in data/pizza_steak_sushi_10_percent already exits, skipping downloading and unzipping...\n",
      "Finished getting data...\n",
      "data/pizza_steak_sushi_20_percent exists...\n",
      "Data in data/pizza_steak_sushi_20_percent already exits, skipping downloading and unzipping...\n",
      "Finished getting data...\n"
     ]
    }
   ],
   "source": [
    "get_data.get_data(\n",
    "    data_dir_str=\"data/\",\n",
    "    image_path_str=\"pizza_steak_sushi_10_percent\",\n",
    "    data_url_str=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "    file_name_str=\"pizza_steak_sushi.zip\"\n",
    ")\n",
    "\n",
    "get_data.get_data(\n",
    "    data_dir_str=\"data/\",\n",
    "    image_path_str=\"pizza_steak_sushi_20_percent\",\n",
    "    data_url_str=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "    file_name_str=\"pizza_steak_sushi_20_percent.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25adb41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory 10%: data/pizza_steak_sushi_10_percent/train\n",
      "Training directory 20%: data/pizza_steak_sushi_20_percent/train\n",
      "Testing directory: data/pizza_steak_sushi_10_percent/test\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_10_percent_path = Path(\"data/pizza_steak_sushi_10_percent\")\n",
    "data_20_percent_path = Path(\"data/pizza_steak_sushi_20_percent\")\n",
    "\n",
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da935b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Create a transform pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), # get image values between 0 & 1\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaec90",
   "metadata": {},
   "source": [
    "## 2. Turn data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd5c85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 8\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)\n",
      "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = setup_data.create_dataloaders(\n",
    "    train_dir=train_dir_10_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create 20% training and test DataLoaders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = setup_data.create_dataloaders(\n",
    "    train_dir=train_dir_20_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045253b",
   "metadata": {},
   "source": [
    "## 3. Exercise 1: Pick a larger model from torchvision.models to add to the list of experiments (for example, EffNetB3 or higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01dcdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb0() -> nn.Module:\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"[INFO]Created EfficientNetB0...\")\n",
    "    return model\n",
    "    \n",
    "def create_effnetb3() -> nn.Module:\n",
    "    weights = torchvision.models.EfficientNet_B3_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b3(weights=weights).to(device)\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1536, out_features=len(class_names), bias=True)\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"[INFO]Created EfficientNetB3...\")\n",
    "    return model\n",
    "\n",
    "def create_effnetb5() -> nn.Module:\n",
    "    weights = torchvision.models.EfficientNet_B5_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b5(weights=weights).to(device)\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(in_features=2048, out_features=len(class_names), bias=True)\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"[INFO]Created EfficientNetB5...\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aab37eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"models\": {\n",
    "        \"effnetb0\": create_effnetb0,\n",
    "        \"effnetb3\": create_effnetb3,\n",
    "        \"effnetb5\": create_effnetb5,\n",
    "    },\n",
    "    \"epochs\": {\n",
    "        \"5\": 5,\n",
    "        \"10\": 10\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"10_percent\": train_dataloader_10_percent,\n",
    "        \"20_percent\": train_dataloader_20_percent\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b7ad075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnetb0\n",
      "effnetb3\n",
      "effnetb5\n"
     ]
    }
   ],
   "source": [
    "for k, v in experiments[\"models\"].items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Created EfficientNetB0...\n",
      "[INFO] Created SummaryWriter, saving to runs/25-08-19/10_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403463ffca5349b7bc722effd022e15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchojna/Documents/GitHub/pytorch-course/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7201 | train_acc: 0.7617 | test_loss: 0.9357 | test_acc: 0.6525\n"
     ]
    }
   ],
   "source": [
    "from src import utils\n",
    "\n",
    "for model_name, model_ in experiments[\"models\"].items():\n",
    "    \n",
    "    for epochs_num, epochs in experiments[\"epochs\"].items():\n",
    "        \n",
    "        for data_name, data in experiments[\"data\"].items():\n",
    "            \n",
    "            model = model_()\n",
    "                    \n",
    "            optimizer = torch.optim.Adam(\n",
    "                params=model.parameters(),\n",
    "                lr=0.001\n",
    "            )\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            train(\n",
    "                model=model,\n",
    "                train_dataloader=data,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                writer=create_writer(\n",
    "                    experiment_name=data_name,\n",
    "                    model_name=model_name,\n",
    "                    extra=f\"{epochs}_epochs\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Save model to file so we can import it later if need be\n",
    "            save_filepath = f\"07_{model_name}_{data_name}_{epochs}_epochs.pth\"\n",
    "            utils.save_model(\n",
    "                model=model,\n",
    "                target_dir=\"models\",\n",
    "                model_name=save_filepath\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aaa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view oru experiments from within notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b4495",
   "metadata": {},
   "source": [
    "## 4. Exercise 2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa74df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0f8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9392b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cacda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51b893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653cf1b1",
   "metadata": {},
   "source": [
    "## Exercise 3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0a9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c34057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6e565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73633715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
