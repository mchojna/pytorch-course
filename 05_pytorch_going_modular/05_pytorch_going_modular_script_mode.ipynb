{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5c6053",
   "metadata": {},
   "source": [
    "# 05. Going Modular: Part 2 (script mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d52c99",
   "metadata": {},
   "source": [
    "## 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3b699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# Setup train and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2abcd",
   "metadata": {},
   "source": [
    "## 2 Create Datasets and DataLoaders (script mode)\n",
    "\n",
    "Let's use the Jupyter magic function to create a `.py` file for creating `DataLoader`s\n",
    "\n",
    "We can save a code cell's contents to a file using the Jupyter `%%writefile filename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6911f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary going_modular scripts\n",
    "import os\n",
    "os.makedirs(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f917c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoader's for image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    \"\"\"Creates training and testing DataLoaders\n",
    "    \n",
    "    Takes in a training directory and testing directory path and turns them into\n",
    "    PyTorch Datasets and then into PyTorch DataLoaders\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Path to training directory\n",
    "        test_dir: Path to testing directory\n",
    "        transform: torchvision.transforms to perform on training and testing data\n",
    "        batch_size: Number of samples per batch in each of the DataLoaders\n",
    "        num_workeres: Integer for number of workers per DataLoader\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names) where class_names\n",
    "        is a list of the target classes\n",
    "        \n",
    "    Example usage:\n",
    "        ```\n",
    "        train_dataloader, test_dataloader, class_names = create_dataloader(\n",
    "            train_dir=path/to/train_dir,\n",
    "            test_dir=path/to/test_dir,\n",
    "            trainsform=some_transform,\n",
    "            batch_size=32,\n",
    "            num_workers=4\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=transform,\n",
    "        target_transform=None,\n",
    "    )\n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=transform,\n",
    "        target_transform=None,\n",
    "    )\n",
    "    \n",
    "    # Get class names as a list\n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    # Turn train and test Datasets into DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf8dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd25d0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1164b7380>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x11649f750>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "test_dataloader, train_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "test_dataloader, train_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22b9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader), len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc1648",
   "metadata": {},
   "source": [
    "## 3. Making a model (TinyVGG) (script mode)\n",
    "\n",
    "Let's turn out model budiling code into a Python script we can import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f936ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a TinyVGG model from CNN Explainer website\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"Creates the TinyVGG architecture.\n",
    "\n",
    "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch  \n",
    "\n",
    "    Args:\n",
    "        input_shape: An integer indicating number of input channels\n",
    "        hidden_units: An integer indicating number of hidden units between layers\n",
    "        output_shape: An integer indicating number of output units\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                    out_channels=hidden_units, \n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc288058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2704, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from going_modular import model_builder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Instantiate a mode from the model_builder.py script\n",
    "model_1 = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=16,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3438f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchojna/Documents/GitHub/pytorch-course/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image shape: torch.Size([1, 3, 64, 64])\n",
      "\n",
      "Output logits:\n",
      "tensor([[ 0.0110, -0.0290,  0.0405]], device='mps:0')\n",
      "\n",
      "Output prediction probabilities:\n",
      "tensor([[0.3344, 0.3213, 0.3444]], device='mps:0')\n",
      "\n",
      "Output prediction label:\n",
      "tensor([2], device='mps:0')\n",
      "\n",
      "Actual label:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 1. Get a batch of images and labels from the DataLoader\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Perform a forward pass on a single image\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_1(img_single.to(device))\n",
    "    \n",
    "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
